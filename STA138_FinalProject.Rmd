---
title: "STA 138 Final Project"
author: "Connor Young"
date: "12/6/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load Data
```{r}
byss = read.csv("./Dataset/Byssinosis.csv")
head(byss)
```

```{r}
# change Workspace to factor
byss$Workspace = factor(byss$Workspace, levels = c("3", "2", "1"))
```

```{r}
# add Total column equal to Byssinosis + Non.Byssinosis
byss$Total = byss$Byssinosis + byss$Non.Byssinosis
```

```{r}
# remove rows which have no total
byss = byss[byss$Total!=0,]
```

```{r}
# unique values of each predictor variable
sapply(byss[1:5], function(x) unique(x))
```

### Logistic Regression Models
#### All Additive Terms and No Interctions
```{r}
glm1 = glm(cbind(Byssinosis, Non.Byssinosis)~Employment + Smoking + Sex + Race + Workspace,
           family=binomial(),
           data=byss)
summary(glm1)
BIC(glm1)
confint.default(glm1)
```

#### Forward Step-Wise Regression
```{r}
result1 = step(glm(cbind(Byssinosis, Non.Byssinosis)~1, binomial(), byss),
                  scope = ~Employment*Smoking*Sex*Race*Workspace,
                  k=log(72),
                  trace=0,
                  direction="forward")
summary(result1)
BIC(result1)
confint.default(result1)
```

#### Compare the Two Models w/ Likelihood Ratio Test
```{r}
# H_0: models are equivalent -> use simpler model
# H_1: models are not equivalent -> use more complex model
anv = anova(result1, glm1)
pvalue = 1-pchisq(anv[2, 4], anv[2,3])
pvalue
```

#### Variable Modifications
```{r}
# changing Workplace var
# 2 & 3 -> 3 (less dusty)
# 1 -> 1 (more dusty)
byss2 = byss
byss2$Workspace[byss2$Workspace == 2] = 3
```

```{r}
# new stepwise w/ Workplace modification
new1 = step(glm(cbind(Byssinosis, Non.Byssinosis)~1, binomial(), byss2),
                  scope = ~Employment*Smoking*Sex*Race*Workspace,
                  k=log(72),
                  trace=0,
                  direction="forward")
summary(new1)
BIC(new1)
confint.default(new1)
1-pchisq(anova(new1, result1)[2, 4], anova(new1, result1)[2,3]) # compare new1 & result1 w/ LRT
```

```{r}
# changing Employment var
# 10-19 & >=20 -> >=10
byss2$Employment[byss2$Employment == "10-19" | byss2$Employment == ">=20"] = ">=10"
```

```{r}
# new stepwise w/ Employment modification
new2 = step(glm(cbind(Byssinosis, Non.Byssinosis)~1, binomial(), byss2),
                  scope = ~Employment*Smoking*Sex*Race*Workspace,
                  k=log(72),
                  trace=0,
                  direction="forward")
summary(new2)
BIC(new2)
confint.default(new2)
1-pchisq(anova(new2, result1)[2, 4], anova(new2, result1)[2,3]) # compare new2 & result1 w/ LRT
1-pchisq(anova(new2, new1)[2, 4], anova(new2, new1)[2,3]) # compare new2 & new1 w/ LRT
```

#### Logistic Regression Diagnostics

To determine how well our model fits the data, we will look at its deviance residuals. A residual is a measure of the difference between an observed value and a predicted value. The most basic residual for our models is the raw difference between the observed value (Byssinosis status: $y_i \in \{0, 1\}$) and the predicted probability ($\hat{\pi_i} \in [0,1]$). This residual will be denoted as $e_i$ and has the form
$$
  e_i = y_i - \hat{\pi_i} \in [-1, 1].
$$
The deviance residual, $d_i$, is based on deviance which is derived from a likelihood ratio test comparing the fitted logistic model and a *saturated* model which fits each data point perfectly. The formula to calculate the deviance residuals is
$$
\begin{aligned}
  d_i &= \text{sign}(e_i)\sqrt{\text{Deviance}} \\
  &= \text{sign}(e_i) \sqrt{-2 \left[ y_i\log\hat{\pi_i} + (1-y_i)\log(1-\hat{\pi_i}) \right]}.
\end{aligned}
$$

The values of these deviance residuals measure how much the predicted probabilities from our model differ from the observed data and whether our model is under- or over-fitting for certain predictor variable values. 

Large values for deviance residuals indicate poor fit and small values indicate good fit. A positive deviance residual indicates our model is under-predicting for a certain set of predictor variable values, i.e., the observed value is 1 and the predicted probability is less than 1. A negative deviance residual indicates our model is over-predicting for a certain set of predictor variable values, i.e., the observed value is 0 and the predicted probability is greater than 0.

A good model fit is indicated by deviance residuals which are evenly distributed and relatively close to 0. Thus, if a smooth line is drawn to "average" the deviance residuals, we would like this line to be relatively flat and close to the line at $d_i=0$.

The plot and interpretation of the deviance residuals for our logistic regression model is shown below.

```{r}
# new2
ry = residuals(new2, type="deviance")
rx = qlogis(fitted.values(new2))

plot(rx, ry,
     xlab = "Fitted Log-Odds",
     ylab = "Deviance Residual",
     main = "Full Additive Model, No Interaction Terms",
     col = "black")
lines(loess.smooth(rx, ry), col="red")
abline(h=0, lty=2, col="grey")
```

From the plot above, we see that the fitted smooth line is relatively straight and close to 0, with a peak around $(-5, 0.2)$ and the rest of the line at $d_i \approx -0.35$. It appears that at low log-odds, this model is  somewhat over-predicting, but at log-odds greater than around -4.5, this model is somewhat under-predicting. Overall, the deviance residuals appear to be relatively equally distributed and the vast majority of the residuals have an absolute value less than 1.5. Thus, we consider this model to be a relatively good fit.




